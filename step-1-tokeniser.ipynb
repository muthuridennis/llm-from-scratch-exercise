{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c63777f-425b-42c5-bc71-1e677ffaf764",
   "metadata": {},
   "source": [
    "## Reading in a short story text sample into Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21dcb7c-044b-4a2e-a27b-54ebb1362939",
   "metadata": {},
   "source": [
    "Reference code from [llms from scratch](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24695aa-2abe-4fbb-848d-b7c7bec17869",
   "metadata": {},
   "source": [
    "### Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d156ad82-f5d1-49a2-a660-1b50f05c9f41",
   "metadata": {},
   "source": [
    "Print out the total number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bb8675d-d083-41e3-97e6-1384ceb42de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "644024b6-1f5a-4f93-870a-40f1c1efa624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of characters ::: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total no. of characters :::\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e108f76-3e8a-467a-9922-25b603fb3527",
   "metadata": {},
   "source": [
    "_**NB:** When working with LLMs it's common to process Gigabytes of text as opposed to the few lines we're working with_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc6381e-2936-4365-8d2c-134cd64a3c4e",
   "metadata": {},
   "source": [
    "Use the split command to perform a common split. We will keep the whitespace as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af330db8-f084-4530-ba4b-6224b252f41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world', ' ', 'this', ' ', 'is', ' ', 'a', ' ', 'text.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world this is a text.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f3cea8-7550-44b8-946f-b5b632a4f677",
   "metadata": {},
   "source": [
    "Now modify this to also include puctuation as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68400e33-57da-42e5-85dd-96801c46e253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', ' ', 'this', ' ', 'is', ' ', 'a', ' ', 'text', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "827ab2fd-aca6-449f-9144-5ed0a9c69447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', 'this', 'is', 'a', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332648b3-7f53-44c4-a9e8-7e5863b69f7c",
   "metadata": {},
   "source": [
    "_**NB:** Removing whitespaces isn't a mandatory thing. There are times where it would be useful to have when training models where whitespace has meaning_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f79ab7-7fb2-49f2-b317-7686939e8b0b",
   "metadata": {},
   "source": [
    "Let's modify the code to now handle other kinds of punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e89f0b9-c5e4-43a2-acfd-1d3ac4592004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "858d10c1-a611-4733-968a-281429c8e2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',raw_text)\n",
    "preprocessed = [item for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e14ddb-f334-442a-9e31-91f6fcb25418",
   "metadata": {},
   "source": [
    "Calculate the total number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbf2a010-ab93-4034-8146-cd535ff7de5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bfc4ad-dc82-4a22-8a70-0caffe7405e8",
   "metadata": {},
   "source": [
    "### Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c30fb-72f4-46b9-94f8-16e18f2f5dc0",
   "metadata": {},
   "source": [
    "We've already tokenised everything and assigned them to a `preprocessed` variable.\n",
    "\n",
    "Now we'll sort the preprocessed words and remove duplicates to allow us to create a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a448463-d2a1-4b70-b080-3c74eb228fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b15595-a532-4aa6-8875-f8293b432e68",
   "metadata": {},
   "source": [
    "Now we create a dictionary of all words and map them to a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19a9cc82-d2bb-44e9-a85c-dc78f26c31ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:index for index, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0e44bf-77ab-46ff-a562-bdc447e3fc52",
   "metadata": {},
   "source": [
    "List the first 50 entries of the dictionary with our token id mapping. This can be used to map new words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b763619a-df82-42d4-b714-f3f89841b38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7854ea-ca1c-4bda-be40-6832f9c78c5c",
   "metadata": {},
   "source": [
    "The process above allows us to encode. Later we'll need a way to map a token id back to the word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5c1989-312c-4585-9239-efc76659780e",
   "metadata": {},
   "source": [
    "A python class with an encode and decode method is used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5e970d5-1313-4536-8819-8621bb9b1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf3741f-0c0e-424e-bcaf-bbcd2ffdd705",
   "metadata": {},
   "source": [
    "Instantiate an instance of the Tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4090f597-22ec-4db2-862f-3f100792e769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "input_text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(input_text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2064f000-a254-49af-9fb0-cef8cd4ede25",
   "metadata": {},
   "source": [
    "Decode the integers back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6cd08d1-ac7b-4566-a1fb-27f6fa76fac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8d04980-f48a-46b6-ad10-9146380e97d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b0d4cc-00cb-4dab-a10b-73bb03c080e5",
   "metadata": {},
   "source": [
    "To make things interesting let's try to include words that aren't in the vocubulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe176c2d-7c65-43a9-8174-155e74aa2d44",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      7\u001b[39m preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.:;?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m,text)\n\u001b[32m      8\u001b[39m preprocessed = [item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m ids = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpreprocessed\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      7\u001b[39m preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.:;?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m,text)\n\u001b[32m      8\u001b[39m preprocessed = [item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e560f4e2-7b93-4f91-9a45-5f478cefa7c0",
   "metadata": {},
   "source": [
    "Hello isn't available so our vocab errors out. LLMs need a way of dealing with this and so will ours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d41226-e6ad-40bb-a254-e10fb1787185",
   "metadata": {},
   "source": [
    "### Adding Special Context Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41480926-faa6-49b7-b530-47d6cc936aff",
   "metadata": {},
   "source": [
    "After implementing the simple tokenizer and realising that it can receive tokens that aren't in it's vocabulary, we need a way to handle that and also tell the LLM when we have reached the end of text.\n",
    "\n",
    "This is done by using special context tokens. For example, a token such as `<|endoftext>` is added to show an end of a sentence while `<|unk>` can be used to show a word that's not in the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a79bfe10-e424-47f2-a090-8c955839a8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(set(preprocessed))\n",
    "# Extend the tokens with the special context tokens\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:index for index, token in enumerate(all_tokens)}\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d058e152-1f99-44dd-b99f-a75d076159ed",
   "metadata": {},
   "source": [
    "Print the last 5 items in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a7b02a0-d9fa-4716-a0b9-1c86418cae82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d453b1d5-0137-4b8b-ba8c-48f66962a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
    "        # Remove the whitespaces\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        # Replace the unknown tokens with a special context token\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "deb5c4cc-8741-4304-a542-0d7a10f95031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text being fed to the encoder >>>>>  Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(\"Text being fed to the encoder >>>>> \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d36c5c3d-571d-469a-89c4-3b467eac66d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01f19a06-45a6-44c7-891d-d9868775c07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0e8510-4ce7-4227-a3ca-449cabb1d1a8",
   "metadata": {},
   "source": [
    "The above shows us that it's possible for us to handle words that don't exist in the original corpus of words. We just augment our vocabulary with these tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c5c68c-c54c-4fff-b859-588e6d73a03e",
   "metadata": {},
   "source": [
    "There are other tokens that can be used such as \n",
    "- BOS - Beginning of Sequence\n",
    "- EOS - End of Sequence\n",
    "- PAD - Padding\n",
    "\n",
    "GPT doesn't use any of the tokens above and only uses `<|endoftext|>` to signify end of sequences and for words which don't exist in the vocabulary, it breaks down words further into byte-pairs which we'll learn next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c70065-2446-4620-a05c-1d53c0a956bc",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding/Tokenisation (BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a761058-cd41-4621-9038-24d0272b898e",
   "metadata": {},
   "source": [
    "This is a subword tokenization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34bbcf-b9f6-4b2c-9db4-3fe8e2190087",
   "metadata": {},
   "source": [
    "As we saw, implementing BPE from scratch can be tasking so an open source library tiktoken will be used. GPT was tokeninsed using this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4dee2dd9-da52-4de5-9d15-9ad68f7f5777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a890d31-352d-4d35-b543-d922e328317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1fb7318b-fc16-4d27-95ea-5bf8beec38fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.11.0\n"
     ]
    }
   ],
   "source": [
    "import importlib.metadata\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df9a0e2-ac14-4f4f-b748-b6bc46003bc6",
   "metadata": {},
   "source": [
    "We now instantiate the tokenizer in the similar way we instantiated the simple word based one we made earlier. I specify the tokenizer for the model I'd like to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "296da001-4990-4cf9-b917-7a3bee8a7dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24e6ff44-bb04-467b-9e5d-bfb5b0748c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d057d8e0-fb1a-4ef1-87ca-5d25093eacae",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fc1d58-deaf-49fa-be54-96fbb87061cc",
   "metadata": {},
   "source": [
    "The BPE tokenizer that trained GPT assigns the `<|endoftext|>` special token the highest character. In this case it is 50256 which shows the size of the token vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1a285b-5d34-4ac9-808d-912e89b844d1",
   "metadata": {},
   "source": [
    "**Here's how the BPE tokenizer allows us to deal with unknown vocabulary tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6e46f34-2d47-47be-9520-90b0a5757e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34431, 322, 268, 1659, 9838, 7211, 9788, 413, 38584]\n",
      "lpowenof yeupp ppewkr\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"lpowenof yeupp ppewkr\")\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87628d65-ce6e-4e38-9572-8bee06f9830c",
   "metadata": {},
   "source": [
    "## Creating Input-Target Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341f54d8-e74d-4fa4-b3a8-709f62e25361",
   "metadata": {},
   "source": [
    "We'll implement a data loader that fetches the input-target pairs using a sliding window approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9271ee2-1ce6-4d23-9995-f597164b3b1c",
   "metadata": {},
   "source": [
    "We first tokenize The Verdict short story we worked with earlier using the BPE algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8db1d5a2-839f-4437-afc3-7ab8e7f6caa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded_text = tokenizer.encode(raw_text)\n",
    "print(len(encoded_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90049ac2-6a1c-4b38-ae40-3e124b984dbf",
   "metadata": {},
   "source": [
    "Shorten the encoded words to first 50 for sampling. We can change this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef3fa83d-a5d5-48fa-a083-76c793fa3e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sample = encoded_text[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a580853-81bc-44f4-92cc-cd5727537c32",
   "metadata": {},
   "source": [
    "A simple way to create input-output pairs would be to create an array of inputs and expected output. We have inputs stored in array X and output in array Y that has the target output shifted by one.\n",
    "\n",
    "When input is `X[l..i]`, out put is `Y[i]` \n",
    "```\n",
    "X = [1, 2, 3, 4]\n",
    "Y = [2, 3, 4, 5]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c222f461-5050-4df1-a965-0665126f0d13",
   "metadata": {},
   "source": [
    "The context size determines how many tokens are included in the input in order to predict the next output. It's the sequence of words/tokens that the model is trained to look at in order to predict the next token/word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "49cc14b4-4bcc-4bd8-92da-a0e7b3b13ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: [290, 4920, 2241, 287]\n",
      "output:     [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 # length of input\n",
    "\n",
    "x = encoded_sample[:context_size]\n",
    "y = encoded_sample[1:context_size + 1] #target output shifted by one\n",
    "\n",
    "print(f\"input: {x}\")\n",
    "print(f\"output:     {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f578c4a9-e62e-417b-ad1d-ad30c1aaa3b1",
   "metadata": {},
   "source": [
    "When we process the inputs along with targets which are just inputs shifted by one position, we can now create the next-word prediction task below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0d2efc42-36ef-4d9f-8347-81bff27cd89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = encoded_sample[:i]\n",
    "    desired = encoded_sample[i]\n",
    "    \n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ff117-0557-4322-b2e8-b123fe792e6f",
   "metadata": {},
   "source": [
    "Everything on the left of the arrow is the input the LLM would receive and then everything on the right would be the token the LLM is supposed to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3407f2f-d188-4f09-abce-7781da4facc8",
   "metadata": {},
   "source": [
    "Now let's take up the same example and decode the output to give a practical feel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "acb6bd29-ec5a-4889-9324-25829f289f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = encoded_sample[:i]\n",
    "    desired = encoded_sample[i]\n",
    "    \n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d4d3c-057a-4b3f-90df-ca8f2b9ad946",
   "metadata": {},
   "source": [
    "Next is to create the input-output pairs in a more efficient and structured way using a better Data Loader than what we have. We'll use PyTorch tensorts which can be thought of as multidimensional arrays. These allow us to run parallel processing since what what we have above not input and output tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1a5b64-5e1a-43c9-8fcb-a7e0aa3035db",
   "metadata": {},
   "source": [
    "### Implementing a Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9bb3de-04ec-4a23-aded-06abeee401f2",
   "metadata": {},
   "source": [
    "DataLoaders allow us to process data in a more efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2cdf79-8eda-4931-86e1-c2f901278e78",
   "metadata": {},
   "source": [
    "To implement efficient dataloaders we collect inputs in a tensor x where each row represents one input context. The second row is simply the input tensore shifted by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a2de57-fb3a-4112-a5db-59998d87ea40",
   "metadata": {},
   "source": [
    "Step 1: Tokenize the entire dataset.\n",
    "\n",
    "Step 2: Use a sliding window to chunk the book into overlapping sequnces of max_length.\n",
    "\n",
    "Step 3: Return the total number of rows in the dataset.\n",
    "\n",
    "Step 4: Return a single row from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "01b5c5b0-08be-4cd4-a9c9-72729941af81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "# Verify if we have PyTorch\n",
    "print(importlib.metadata.version(\"torch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80ce2374-5482-4d1e-81b8-a8eaf85ee4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    # stride determines how much we slide.\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        #Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        #Use a sliding window to chuch the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            output_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(output_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    # Implemented for PyTorch Dataloader to use when loading into the Dataloader\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd4a2d3-b7d8-4950-b79e-facfc3a49116",
   "metadata": {},
   "source": [
    "The number of tokens in each row is equal to the context window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770cee2a-7f2e-47a2-9e23-6fa4afd25566",
   "metadata": {},
   "source": [
    "Now we use the GPTDatasetV1 to load the inputs in batches via a PyTorch DataLoader:\n",
    "\n",
    "Step 1: Initialize the tokenizer\n",
    "\n",
    "Step 2: Create dataset\n",
    "\n",
    "Step 3: drop_last=True drops the last batch if it's shorter than the specified batch_size to prevent loss spikes during training.\n",
    "\n",
    "Step 4: The number of CPU processes to use for pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8c8a8810-3ce9-4773-a05d-4caff71d5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True, \n",
    "                         num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253d8a5b-3475-4302-8422-c37cd4faecdc",
   "metadata": {},
   "source": [
    "Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4 to see how the `GPTDataSetV1` and the `create_dataloader_v1` method works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d5df0942-12f4-4e8a-8747-a8ef27949620",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d15796-c057-4ff7-99c9-badc6b1e8c06",
   "metadata": {},
   "source": [
    "Convert dataloader into a Python iterator to fetch the next entry via Python's built in `next()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb81458-a0ab-4f39-a11f-440116655547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader :::  <torch.utils.data.dataloader.DataLoader object at 0x10d80d890>\n",
      "first_batch ::::  [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "second_batch ::::  [tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "context_length = 4\n",
    "\n",
    "# Strider of 1 is for demonstration purposes. This should be set to the context_length to avoid overfitting.\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=context_length, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Dataloader ::: \", dataloader)\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "first_batch = next(data_iter)\n",
    "print(\"first_batch :::: \", first_batch)\n",
    "\n",
    "second_batch = next(data_iter)\n",
    "print(\"second_batch :::: \", second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8490cd4e-0ed3-4e13-986a-b3f931120bb2",
   "metadata": {},
   "source": [
    "Batch size is modified depending on training demands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4fd113",
   "metadata": {},
   "source": [
    "We can also create batched outputs by increasing the batch_size. If we increase batch_size we can also increase stride so that we don't have overlaps between batches, since more overlap could lead to increased overfitting. The stride is usually kept at the same length as the context_length to ensure that we don't miss anywords and we avoid overlapping and overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ec7328c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30759ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-from-scratch-exercise-TlXelRaM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
