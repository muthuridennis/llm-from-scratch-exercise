{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f8b346",
   "metadata": {},
   "source": [
    "## Reading in a short story text sample into Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e5dc87",
   "metadata": {},
   "source": [
    "Reference code from [llms from scratch](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817d92f8",
   "metadata": {},
   "source": [
    "## Vector Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dcaeb7",
   "metadata": {},
   "source": [
    "### Using pre-trained token embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d1b918",
   "metadata": {},
   "source": [
    "Import a Trained Model using [gensim](https://radimrehurek.com/gensim/). Then load the [word2vec-google-news-300](https://huggingface.co/fse/word2vec-google-news-300) vector model from Google which has already been pretrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ace8aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After trying a number of times to download the model directly via api.load('word2vec-google-news-300').\n",
    "I downloaded it directly from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing in this local folder\n",
    "\"\"\"\n",
    "\n",
    "# import gensim.downloader as api\n",
    "# import time\n",
    "\n",
    "# model = None\n",
    "# retries = 5\n",
    "# delay_seconds = 5\n",
    "\n",
    "# for attempt in range(retries):\n",
    "#     try:\n",
    "#         print(f\"Attempt {attempt + 1} to load word2vec-google-news-300...\")\n",
    "#         model = api.load('word2vec-google-news-300')\n",
    "#         print(\"Model loaded successfully.\")\n",
    "#         break  # Exit loop if successful\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading model: {e}\")\n",
    "#         if attempt < retries - 1:\n",
    "#             print(f\"Retrying in {delay_seconds} seconds...\")\n",
    "#             time.sleep(delay_seconds)\n",
    "#         else:\n",
    "#             print(\"Max retries reached. Could not load model.\")\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e155f5a8",
   "metadata": {},
   "source": [
    "We can get the vector representation of the word dog from the Google News model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc1008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.12695312e-02 -2.23388672e-02 -1.72851562e-01  1.61132812e-01\n",
      " -8.44726562e-02  5.73730469e-02  5.85937500e-02 -8.25195312e-02\n",
      " -1.53808594e-02 -6.34765625e-02  1.79687500e-01 -4.23828125e-01\n",
      " -2.25830078e-02 -1.66015625e-01 -2.51464844e-02  1.07421875e-01\n",
      " -1.99218750e-01  1.59179688e-01 -1.87500000e-01 -1.20117188e-01\n",
      "  1.55273438e-01 -9.91210938e-02  1.42578125e-01 -1.64062500e-01\n",
      " -8.93554688e-02  2.00195312e-01 -1.49414062e-01  3.20312500e-01\n",
      "  3.28125000e-01  2.44140625e-02 -9.71679688e-02 -8.20312500e-02\n",
      " -3.63769531e-02 -8.59375000e-02 -9.86328125e-02  7.78198242e-03\n",
      " -1.34277344e-02  5.27343750e-02  1.48437500e-01  3.33984375e-01\n",
      "  1.66015625e-02 -2.12890625e-01 -1.50756836e-02  5.24902344e-02\n",
      " -1.07421875e-01 -8.88671875e-02  2.49023438e-01 -7.03125000e-02\n",
      " -1.59912109e-02  7.56835938e-02 -7.03125000e-02  1.19140625e-01\n",
      "  2.29492188e-01  1.41601562e-02  1.15234375e-01  7.50732422e-03\n",
      "  2.75390625e-01 -2.44140625e-01  2.96875000e-01  3.49121094e-02\n",
      "  2.42187500e-01  1.35742188e-01  1.42578125e-01  1.75781250e-02\n",
      "  2.92968750e-02 -1.21582031e-01  2.28271484e-02 -4.76074219e-02\n",
      " -1.55273438e-01  3.14331055e-03  3.45703125e-01  1.22558594e-01\n",
      " -1.95312500e-01  8.10546875e-02 -6.83593750e-02 -1.47094727e-02\n",
      "  2.14843750e-01 -1.21093750e-01  1.57226562e-01 -2.07031250e-01\n",
      "  1.36718750e-01 -1.29882812e-01  5.29785156e-02 -2.71484375e-01\n",
      " -2.98828125e-01 -1.84570312e-01 -2.29492188e-01  1.19140625e-01\n",
      "  1.53198242e-02 -2.61718750e-01 -1.23046875e-01 -1.86767578e-02\n",
      " -6.49414062e-02 -8.15429688e-02  7.86132812e-02 -3.53515625e-01\n",
      "  5.24902344e-02 -2.45361328e-02 -5.43212891e-03 -2.08984375e-01\n",
      " -2.10937500e-01 -1.79687500e-01  2.42187500e-01  2.57812500e-01\n",
      "  1.37695312e-01 -2.10937500e-01 -2.17285156e-02 -1.38671875e-01\n",
      "  1.84326172e-02 -1.23901367e-02 -1.59179688e-01  1.61132812e-01\n",
      "  2.08007812e-01  1.03027344e-01  9.81445312e-02 -6.83593750e-02\n",
      " -8.72802734e-03 -2.89062500e-01 -2.14843750e-01 -1.14257812e-01\n",
      " -2.21679688e-01  4.12597656e-02 -3.12500000e-01 -5.59082031e-02\n",
      " -9.76562500e-02  5.81054688e-02 -4.05273438e-02 -1.73828125e-01\n",
      "  1.64062500e-01 -2.53906250e-01 -1.54296875e-01 -2.31933594e-02\n",
      " -2.38281250e-01  2.07519531e-02 -2.73437500e-01  3.90625000e-03\n",
      "  1.13769531e-01 -1.73828125e-01  2.57812500e-01  2.35351562e-01\n",
      "  5.22460938e-02  6.83593750e-02 -1.75781250e-01  1.60156250e-01\n",
      " -5.98907471e-04  5.98144531e-02 -2.11914062e-01 -5.54199219e-02\n",
      " -7.51953125e-02 -3.06640625e-01  4.27734375e-01  5.32226562e-02\n",
      " -2.08984375e-01 -5.71289062e-02 -2.09960938e-01  3.29589844e-02\n",
      "  1.05468750e-01 -1.50390625e-01 -9.37500000e-02  1.16699219e-01\n",
      "  6.44531250e-02  2.80761719e-02  2.41210938e-01 -1.25976562e-01\n",
      " -1.00585938e-01 -1.22680664e-02 -3.26156616e-04  1.58691406e-02\n",
      "  1.27929688e-01 -3.32031250e-02  4.07714844e-02 -1.31835938e-01\n",
      "  9.81445312e-02  1.74804688e-01 -2.36328125e-01  5.17578125e-02\n",
      "  1.83593750e-01  2.42919922e-02 -4.31640625e-01  2.46093750e-01\n",
      " -3.03955078e-02 -2.47802734e-02 -1.17187500e-01  1.61132812e-01\n",
      " -5.71289062e-02  1.16577148e-02  2.81250000e-01  4.27734375e-01\n",
      "  4.56542969e-02  1.01074219e-01 -3.95507812e-02  1.77001953e-02\n",
      " -8.98437500e-02  1.35742188e-01  2.08007812e-01  1.88476562e-01\n",
      " -1.52343750e-01 -2.37304688e-01 -1.90429688e-01  7.12890625e-02\n",
      " -2.46093750e-01 -2.61718750e-01 -2.34375000e-01 -1.45507812e-01\n",
      " -1.17187500e-02 -1.50390625e-01 -1.13281250e-01  1.82617188e-01\n",
      "  2.63671875e-01 -1.37695312e-01 -4.58984375e-01 -4.68750000e-02\n",
      " -1.26953125e-01 -4.22363281e-02 -1.66992188e-01  1.26953125e-01\n",
      "  2.59765625e-01 -2.44140625e-01 -2.19726562e-01 -8.69140625e-02\n",
      "  1.59179688e-01 -3.78417969e-02  8.97216797e-03 -2.77343750e-01\n",
      " -1.04980469e-01 -1.75781250e-01  2.28515625e-01 -2.70996094e-02\n",
      "  2.85156250e-01 -2.73437500e-01  1.61132812e-02  5.90820312e-02\n",
      " -2.39257812e-01  1.77734375e-01 -1.34765625e-01  1.38671875e-01\n",
      "  3.53515625e-01  1.22070312e-01  1.43554688e-01  9.22851562e-02\n",
      "  2.29492188e-01 -3.00781250e-01 -4.88281250e-02 -1.79687500e-01\n",
      "  2.96875000e-01  1.75781250e-01  4.80957031e-02 -3.38745117e-03\n",
      "  7.91015625e-02 -2.38281250e-01 -2.31445312e-01  1.66015625e-01\n",
      " -2.13867188e-01 -7.03125000e-02 -7.56835938e-02  1.96289062e-01\n",
      " -1.29882812e-01 -1.05957031e-01 -3.53515625e-01 -1.16699219e-01\n",
      " -5.10253906e-02  3.39355469e-02 -1.43554688e-01 -3.90625000e-03\n",
      "  1.73828125e-01 -9.96093750e-02 -1.66015625e-01 -8.54492188e-02\n",
      " -3.82812500e-01  5.90820312e-02 -6.22558594e-02  8.83789062e-02\n",
      " -8.88671875e-02  3.28125000e-01  6.83593750e-02 -1.91406250e-01\n",
      " -8.35418701e-04  1.04003906e-01  1.52343750e-01 -1.53350830e-03\n",
      "  4.16015625e-01 -3.32031250e-02  1.49414062e-01  2.42187500e-01\n",
      " -1.76757812e-01 -4.93164062e-02 -1.24511719e-01  1.25976562e-01\n",
      "  1.74804688e-01  2.81250000e-01 -1.80664062e-01  1.03027344e-01\n",
      " -2.75390625e-01  2.61718750e-01  2.46093750e-01 -4.71191406e-02\n",
      "  6.25000000e-02  4.16015625e-01 -3.55468750e-01  2.22656250e-01]\n"
     ]
    }
   ],
   "source": [
    "dog = model.get_vector(key=\"dog\")\n",
    "\n",
    "print(dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de00540",
   "metadata": {},
   "source": [
    "To get the number of dimensions it was trained on, we use shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f75501",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0410e189",
   "metadata": {},
   "source": [
    "Let's try find similar words\n",
    "\n",
    "If we have `king + woman - man` as a vector operation. We should have a result that matches queen or woman. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f03b53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118193507194519), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321839332581), ('kings', 0.5236844420433044), ('Queen_Consort', 0.5235945582389832), ('queens', 0.5181134343147278), ('sultan', 0.5098593831062317), ('monarchy', 0.5087411999702454)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['king', 'woman'], negative=['man'], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d62b0fb",
   "metadata": {},
   "source": [
    "### Creating our own token embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0f62d6",
   "metadata": {},
   "source": [
    "Suppose we have input tokens with the ids below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "235655a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e72197",
   "metadata": {},
   "source": [
    "For simplicity, we'll create a token embedding with a vocabulary size of 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfdeef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6 #rows\n",
    "output_dim = 3 #columns\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ad8742",
   "metadata": {},
   "source": [
    "Let's view the embedding layers weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061ef300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e859f5b5",
   "metadata": {},
   "source": [
    "The embedding layer contains a row for each vocabulary and a column for each dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3344afad",
   "metadata": {},
   "source": [
    "We can apply a token id to obtain the embedding vector. This becomes a map where we can access the row number using the token id as the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a1f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e0e55",
   "metadata": {},
   "source": [
    "Let's now retrieve 3D vector embedding representations of multiple token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d166a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef156e8",
   "metadata": {},
   "source": [
    "## Positional Encoding/Embedding (Encoding Word Positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b09bc66",
   "metadata": {},
   "source": [
    "We'll now use a 256 embedding size(columns), a 50257 token id size(rows) to perform encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5b02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "# randomly initialised layer\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1dcec8",
   "metadata": {},
   "source": [
    "Next we'll sample data from a DataLoader and process the data in 8 batches. This means that parameters will be updated after going through 8 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0765dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loader from Previous example\n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    # stride determines how much we slide.\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        #Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        #Use a sliding window to chuch the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            output_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(output_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    # Implemented for PyTorch Dataloader to use when loading into the Dataloader\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "    \n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True, \n",
    "                         num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "## Import the raw text\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a57ca5a",
   "metadata": {},
   "source": [
    "Now we instantiate the data loader which samples data in a sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de4a224e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Inputs Shape torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "context_length = 4\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=context_length, \n",
    "    stride=context_length, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Token IDs\", inputs) \n",
    "print(\"Inputs Shape\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85ecf759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcd9065",
   "metadata": {},
   "source": [
    "As seen from the shape above, each token is now a 256 dimensional vector.\n",
    "\n",
    "For GPT absolute embedding approach we just need to create another embedding layer that has the same context length.\n",
    "\n",
    "We only need to do positional embedding once. All other  after which the positional embedding vector will be retrieved once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3357943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = 4\n",
    "output_dim = 256\n",
    "\n",
    "# randomly initialised layer\n",
    "position_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "position_embeddings = position_embedding_layer(torch.arange(context_length))\n",
    "\n",
    "print(position_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c4434f",
   "metadata": {},
   "source": [
    "Add token embeddings to position embeddings as the Position embedding pattern demands. The 8x4x256 can be added to the 4x256 matrix by performing broadcasting which replicates the 4x256 for each of the 8 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac915b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1951, -0.5051,  1.0742,  ...,  2.0425, -0.7197,  0.9533],\n",
      "        [ 0.2950, -0.5241,  0.9449,  ...,  0.0627,  0.6589, -2.6676],\n",
      "        [-0.3345,  0.0423,  0.3279,  ...,  1.9987,  1.1475, -0.8502],\n",
      "        [-2.4531,  0.4572,  0.6748,  ...,  1.8094, -0.2438, -1.3124]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "final_vector_embedding_output = token_embeddings + position_embeddings\n",
    "print(final_vector_embedding_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2ede7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-from-scratch-ck85bdpM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
